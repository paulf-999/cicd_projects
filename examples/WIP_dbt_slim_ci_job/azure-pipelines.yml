---
variables:

    # the source branch from the Git PR (pull request)
    SOURCE_GIT_BRANCH_NAME: $[replace(variables['System.PullRequest.SourceBranch'], 'refs/heads/', '')]

    # filepath to where the CICD files are stored within the git repo
    CICD_FILES_DIR: src/cicd
    # dbt variables
    DBT_PROJECT_DIR: example
    DBT_PROFILE_DIR: profiles
    DBT_PROD_RUN_ARTIFACTS_DIR: dbt_prod_run_artifacts
    # Azure Key Vault name
    AZURE_KEY_VAULT_NAME: <TODO>

# only trigger the CI job on PR (pull request) to the master branch
pr:
    branches:
        include:
            - feature/cicd_sqlfluff_dev
            # - master

pool: <TODO_AZ_AGENT>

jobs:
    #======================================================================================================================================
    # Job: dbt 'slim CI' job (i.e., run modified & downstream models only)
    # Prerequisite: to generated a value for 'state:modified', we require two manifest.json files for comparison - master vs feature branch
    #======================================================================================================================================
    - job: dbtSlimCIJob
      displayName: dbt 'Slim CI' job
      steps:
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Note: the first 3 steps below (i.e., before 'Step 1') are to be removed once the AZ agent upgrade has been made.
          #--------------------------------------------------------------------------------------------------------------------------------------
          - script: rm -rf /usr/local/lib/python3.8/dist-packages/OpenSSL && rm -rf /usr/local/lib/python3.8/dist-packages/pyOpenSSL-22.1.0.dist-info/
            displayName: Prep step/too be removed - Remove the OpenSSL package files (it's causing issues)
          - script: pip install --upgrade pyOpenSSL
            displayName: Prep step/too be removed - Upgrade pyOpenSSL
          - script: pip install -r $(CICD_FILES_DIR)/requirements.txt
            displayName: Prep step/to be removed - pip install requirements.txt
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 1: Connect to Azure Key Vault (KV) and fetch the Vault secrets we're interested in
          #--------------------------------------------------------------------------------------------------------------------------------------
          #<TODO>
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 2: Generate a timestamp var ($DATETIME_TS) to use as a unique schema name
          #--------------------------------------------------------------------------------------------------------------------------------------
          - script: |
                echo && echo "Generate a datetime timestamp var - this is then used in the dbt profile render step (below)" && echo
                date=$(date +%Y%m%d_%H%M%S)
                echo "##vso[task.setvariable variable=DATETIME_TS;]$date"
            displayName: Generate timestamp var (i.e., $DATETIME_TS) for dynamic schema name
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 3: Using the above KV secrets, let's render the dbt 'QA' profiles.yml file
          #--------------------------------------------------------------------------------------------------------------------------------------
          - script: |
                echo && echo "##[section]Step 1: Render the dbt profiles.yml template."
                echo "##[debug]Command = j2 src/templates/profiles.yml.j2 -o $(DBT_PROFILE_DIR)/profiles.yml"
                j2 src/templates/profiles.yml.j2 -o $(DBT_PROFILE_DIR)/profiles.yml
                echo "##[section]Step 2: Print out the (masked) contents of the generated profiles.yml file" && echo
                cat $(DBT_PROFILE_DIR)/profiles.yml && echo
            env:
                SNOWFLAKE_ACCOUNT_NAME: $(SNOWFLAKE-ACCOUNT-NAME)
                SNOWFLAKE_ROLE_QA: $(SNOWFLAKE-ROLE)
                SNOWFLAKE_DBT_USERNAME_QA: $(SNOWFLAKE-DBT-USERNAME)
                SNOWFLAKE_DATABASE_QA: $(SNOWFLAKE-DATABASE)
                DATETIME_STRING: $(DATETIME_TS) # Created in the above step "Timestamp VAR Creation (i.e., $DATETIME_TS)"
                SNOWFLAKE_SCHEMA_PREFIX_QA: $(SNOWFLAKE-SCHEMA-PREFIX)
            displayName: Render dbt profiles.yml
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 4. In case the job doesn't complete & we want to troubleshoot the job failures, let's push the compiled (dbt) SQL files to the azure pipeline assets
          #--------------------------------------------------------------------------------------------------------------------------------------
          # copy the dbt generated artefacts (i.e., those stored within the dbt 'target' directory)
          # though to generate the compiled dbt models, we'll first need to compile our dbt project
          - script: cd $(DBT_PROJECT_DIR) && dbt --quiet compile --profiles-dir=profiles
            env:
                SNOWFLAKE_DBT_PASSWORD_QA: $(SNOWFLAKE-DBT-PASSWORD)
            displayName: Troubleshooting - run 'dbt compile' to generate the rendered dbt compilation files
          # part 1: copy the compiled dbt models
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/target/compiled/prod_analytics/**
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy dbt target/compiled artefacts (target/run/prod_analytics)
          # part 2: copy the dbt generated logs
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/logs/**
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy dbt logs
          # publish the logs & compiled dbt models to the azure pipeline assets
          - task: PublishBuildArtifacts@1
            inputs:
                PathtoPublish: $(Build.ArtifactStagingDirectory)
                ArtifactName: dbt_job_run_artifacts
                publishLocation: Container
            displayName: Publish the dbt logs & artifacts
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 5: Run modified (and downstream) dbt models
          #--------------------------------------------------------------------------------------------------------------------------------------
          - task: Bash@3
            inputs:
                filePath: $(CICD_FILES_DIR)/dbt_slim_ci_job.sh
                arguments: $(SOURCE_GIT_BRANCH_NAME)
            env:
                SNOWFLAKE_DBT_PASSWORD_QA: $(SNOWFLAKE-DBT-PASSWORD)
            displayName: === Main dbt 'Slim CI' job logic === (see src/cicd/dbt_slim_ci_job/dbt_slim_ci_job.sh)
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 6: Copy the dbt logs & the dbt run artefacts (i.e., those stored within the dbt 'target' directory)
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Part 1: copy the dbt generated logs
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/logs/**
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy dbt logs

          # Part 2: copy the dbt run artefacts (i.e., those stored within the dbt 'target' directory)
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/target/run/prod_analytics/**
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy dbt target/run artefacts (target/run/prod_analytics)

          # Part 2: copy the list of modified dbt models identified
          - task: CopyFiles@2
            inputs:
                Contents: $(DBT_PROJECT_DIR)/dbt_modified_models.txt
                targetFolder: $(Build.ArtifactStagingDirectory)
            displayName: Copy the list of modified dbt models identified
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Step 7. Let's now push all of our dbt assets (logs & compiled/'run' dbt models) up to AZ artefacts
          #--------------------------------------------------------------------------------------------------------------------------------------
          # Let's again push the dbt generated logs up to AZ artefacts
          - task: PublishBuildArtifacts@1
            inputs:
                PathtoPublish: $(Build.ArtifactStagingDirectory)
                ArtifactName: dbt_job_run_artifacts
                publishLocation: Container
            displayName: Publish the dbt logs & artifacts
